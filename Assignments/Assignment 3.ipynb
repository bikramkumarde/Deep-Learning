{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network on MNIST Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run each cell in the notebook. The explanation of the cells are given on top respectively. Also, you need to have keras with tensorflow backend to load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras_team.keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloading the Mnist data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling the data so that values are between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) (60000,)\n",
      "(10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape, y_train.shape)\n",
    "print(x_test.shape, y_test.shape)\n",
    "x_train = x_train/255\n",
    "x_test = x_test/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the training and testing dataset alongwith setting the Y values to 0 or 1 depending on a condition. The condition here is last digit of VUNetId. My last digit is 1. So for the Y values of 1, new value is set to 1(true). Rest of them are set to 0(false). Also the 28x28 dataset is converted to a column vector 784x1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = x_train.reshape(60000, 784)  #Converting to column vector for each datapoint\n",
    "X1 = X\n",
    "Y = np.array(y_train == 1)\n",
    "Y = 1*Y    #Setting values to 1 or 0 for training\n",
    "\n",
    "test_X = x_test.reshape(10000,784)\n",
    "test_X = test_X.transpose()\n",
    "test_Y = np.array(y_test == 1)\n",
    "test_Y = 1*test_Y         # Setting values to 1 or 0 for test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the forward propagation step of the hidden layer. Here W, b and the data is received as input. The operation W.X + b is performed after taking appropriate transpose. The final value is then passed through a ReLu or tanh function according to our wishes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop_hidden(W, b, data_X):\n",
    "    Wt = W.transpose()\n",
    "    intermediate = np.inner(Wt, data_X.transpose())\n",
    "    Z = intermediate + b\n",
    "    A = relu(Z)\n",
    "    return A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the forward propagation step of the output layer. Here W, b and the data is received as input. The operation W.X + b is performed after taking appropriate transpose. The final value is then passed through a sigmoid function to make the value between 0 - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop_output(W, b, data_X):\n",
    "    Wt = W.transpose()\n",
    "    intermediate = np.inner(Wt, data_X.transpose())\n",
    "    Z = intermediate + b\n",
    "    A = sigmoid(Z)\n",
    "    return A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the backward propagation step. Here the required derivatives are calculated which will be later updated in further steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_prop_1(A1, m, W2, dZ2, data_X):\n",
    "    dZ1 = np.inner(W2, dZ2.transpose())\n",
    "    dZ1 = dZ1* relu_derivative(dZ1)\n",
    "    dW1 = np.inner(data_X, dZ1)/m\n",
    "    db1 = sum(dZ1)/m\n",
    "    return dW1, db1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the backward propagation of output layer(Layer2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_prop_2(A2, A1, m, data_Y):\n",
    "    dZ2 = A2 - data_Y\n",
    "    dW2 = np.inner(dZ2, A1)/m\n",
    "    db2 = sum(dZ2)/m\n",
    "    return dW2, db2, dZ2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the sigmoid activation function used to map any real value between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the Rectified Linear Unit(ReLU) activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the derivative of the ReLU function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_derivative(x):\n",
    "    y = np.array(x > 0)\n",
    "    y = 1*y\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the tanh activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(x):\n",
    "    return np.tanh(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the derivative of tanh function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh_derivative(x):\n",
    "    return 1/np.square(np.cosh(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computes the cost function for all the training samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function(A, m, data_Y):\n",
    "    total_cost = -(1 / m) * np.sum(data_Y * np.log(A) + (1 - data_Y) * np.log(1 - A))\n",
    "    return total_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the results are predicted. The W and b of model are taken as input. The values are predicted using test dataset and then using the actual values the accuracy is computed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def results(W1, b1, W2, b2, data_X, data_Y):\n",
    "    W1t = W1.transpose()\n",
    "    W2t = W2.transpose()\n",
    "    A1 = forward_prop_hidden(W1, b1, data_X)\n",
    "    A2 = forward_prop_output(W2, b2, A1)\n",
    "    pred_Y = A2.transpose()\n",
    "    pred_Y = np.around(pred_Y, decimals = 0)   #Rounding off the predicted value to 0 or 1\n",
    "    accuracy = accuracy_score(data_Y, pred_Y)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "This is the neural network function. First we split the training data into training and validation sets. Then we initialize W and b with random numbers. After that using W, b and the training data we implement the forward propagation. Using the results we calculate the derivatives for the backward propagation step. After that we update our b and W using the derivative values obtained. We then test this using the validation data. We store the alpha for which accuracy was the highest. We also check with different number of hidden units which can be set from the variable h."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Epoch: 500\n",
      "Epoch: 1000\n",
      "Alpha: 0.0001 Accuracy: 0.8868333333333334\n",
      "Epoch: 0\n",
      "Epoch: 500\n",
      "Epoch: 1000\n",
      "Alpha: 0.001 Accuracy: 0.8945\n",
      "Epoch: 0\n",
      "Epoch: 500\n",
      "Epoch: 1000\n",
      "Alpha: 0.005 Accuracy: 0.9133333333333333\n",
      "Epoch: 0\n",
      "Epoch: 500\n",
      "Epoch: 1000\n",
      "Alpha: 0.01 Accuracy: 0.9718333333333333\n",
      "Epoch: 0\n",
      "Epoch: 500\n",
      "Epoch: 1000\n",
      "Alpha: 0.1 Accuracy: 0.15658333333333332\n",
      "Epoch: 0\n",
      "Epoch: 500\n",
      "Epoch: 1000\n",
      "Alpha: 0.05 Accuracy: 0.98125\n",
      "Max: 0.05\n"
     ]
    }
   ],
   "source": [
    "def neural_net():    \n",
    "    alpha = [0.0001, 0.001, 0.005, 0.01, 0.1, 0.05]   #set of learning rates to check from\n",
    "    epochs = 1001\n",
    "    max_cost = 0\n",
    "    max_accuracy = 0\n",
    "    alpha_max = 0\n",
    "    h = 5            #hidden units\n",
    "    for i in range(len(alpha)):\n",
    "        train_X, valid_X, train_Y, valid_Y = train_test_split(X, Y, test_size=0.2) # Splitting training data using 80:20 rule\n",
    "        valid_X = valid_X.transpose()\n",
    "        train_X = train_X.transpose()\n",
    "        n, m = train_X.shape\n",
    "        W1 = np.random.rand(n,h)     #Initializing W and b\n",
    "        b1 = random.random()\n",
    "        W2 = np.random.rand(h,1)\n",
    "        b2 = random.random()\n",
    "        for j in range(epochs):\n",
    "            if(j%500 == 0):\n",
    "                print('Epoch:', j)\n",
    "            A1 = forward_prop_hidden(W1, b1, train_X)         #forward propagation through hidden layer\n",
    "            A2 = forward_prop_output(W2, b2, A1)\n",
    "            dW2, db2, dZ2 = backward_prop_2(A2, A1, m, train_Y)        #backward propagation through output layer\n",
    "            dW1, db1 = backward_prop_1(A1, m, W2, dZ2, train_X)\n",
    "            W1 = W1 - alpha[i]*dW1                    #Updating b and W\n",
    "            b1 = b1 - alpha[i]*db1\n",
    "            W2 = W2 - alpha[i]*dW2.transpose()\n",
    "            b2 = b2 - alpha[i]*db2\n",
    "        b1 = sum(b1)/m\n",
    "        b2 = sum(b2)/m\n",
    "        accuracy = results(W1, b1, W2, b2, valid_X, valid_Y)   #testing the accuracy using validation set\n",
    "        if(accuracy >= max_accuracy):\n",
    "            alpha_max = alpha[i]                     #finding the alpha for which accuracy was the maximum\n",
    "            max_accuracy = accuracy\n",
    "        print('Alpha:',alpha[i],'Accuracy:', accuracy)\n",
    "    print( 'Max:',alpha_max)\n",
    "    return alpha_max\n",
    "alpha_max = neural_net()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we calculate the cost function for each iteration using the learning rate which we had obtained earlier. We run this on the entire training set. After obatining values of W and b, we calculate the training and test error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bikram Kumar De\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: RuntimeWarning: divide by zero encountered in log\n",
      "  \n",
      "C:\\Users\\Bikram Kumar De\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: RuntimeWarning: invalid value encountered in multiply\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 500\n",
      "Epoch: 1000\n",
      "Epoch: 1500\n",
      "Epoch: 2000\n",
      "Training error: 2.541666666666657 %\n",
      "Test error: 2.3800000000000097 %\n"
     ]
    }
   ],
   "source": [
    "X = X.transpose()\n",
    "n, m = X.shape\n",
    "h = 5  #no of hidden units\n",
    "W1 = np.random.rand(n,h)     #Initializing W and b\n",
    "b1 = random.random()\n",
    "W2 = np.random.rand(h,1)\n",
    "b2 = random.random()\n",
    "epochs = 2001\n",
    "cost_array = []\n",
    "J_arr =[]\n",
    "count = 0\n",
    "for j in range(epochs):\n",
    "    if(j%500 == 0):\n",
    "        print('Epoch:', j)\n",
    "    A1 = forward_prop_hidden(W1, b1, X)\n",
    "    A2 = forward_prop_output(W2, b2, A1)\n",
    "    dW2, db2, dZ2 = backward_prop_2(A2, A1, m, Y)\n",
    "    dW1, db1 = backward_prop_1(A1, m, W2, dZ2, X)\n",
    "    W1 = W1 - alpha_max*dW1                    #Updating b and W\n",
    "    b1 = b1 - alpha_max*db1\n",
    "    W2 = W2 - alpha_max*dW2.transpose()\n",
    "    b2 = b2 - alpha_max*db2\n",
    "    total_cost = cost_function(A2, m, Y)\n",
    "    cost_array.append(total_cost)    # Storing the value of cost function\n",
    "    J_arr.append(count)\n",
    "    count+= 1\n",
    "b1 = sum(b1)/m\n",
    "b2 = sum(b2)/m\n",
    "final_accuracy_test = results(W1, b1, W2, b2, test_X, test_Y)*100    # Calculating the training and test accuracy\n",
    "final_accuracy_train = results(W1, b1, W2, b2, X1.transpose(), Y)*100\n",
    "print('Training error:', str(100-final_accuracy_train),'%')\n",
    "print('Test error:', str(100-final_accuracy_test),'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the cost function as a function of the iterations. We can see that the cost function decreases very fast initially but after a certain number of iterations the rate of decrease is much less which indicates that our neural network is  converging hence the error is reducing asymptotically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title('Cost Function Graph')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Cost Function')\n",
    "plt.plot(J_arr, cost_array)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, you can check any individual image. You can load the particular image from the test dataset by changing the array index. The program will tell whether the number is 1 or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = x_test[2]\n",
    "plt.imshow(x1, interpolation='nearest', cmap = 'gray')\n",
    "plt.show()\n",
    "x1 = x1.reshape(784, 1)\n",
    "A1 = forward_prop_hidden(W1, b1, x1)\n",
    "A2 = forward_prop_output(W2, b2, A1)\n",
    "if (A2<0.5):\n",
    "    print('This is not 1')\n",
    "else:\n",
    "    print('This is 1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is not 1\n"
     ]
    }
   ],
   "source": [
    "x1 = x_test[2566]\n",
    "plt.imshow(x1, interpolation='nearest', cmap = 'gray')\n",
    "plt.show()\n",
    "x1 = x1.reshape(784, 1)\n",
    "A1 = forward_prop_hidden(W1, b1, x1)\n",
    "A2 = forward_prop_output(W2, b2, A1)\n",
    "if (A2<0.5):\n",
    "    print('This is not 1')\n",
    "else:\n",
    "    print('This is 1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Higher learning rate means the regression learns faster, i.e. it converges faster. The accuracy at end of 1000 epochs may not show significant changes on changing the learning rate because in this small dataset, having smaller alphas also makes the program converge.\n",
    "I have also checked the impact of different no of hidden layers by changing the value of h. It changes the accuracy but not significantly. It decreases the accuracy a bit. But it might also increase it. I found two different values when I ran the code twice with different hidden units. Anyway, the change was insignificant and might have been caused by the initial values of W and b.\n",
    "Also, I have checked the neural network using two different activation function tanh and ReLU in the hidden layer. To check it now, we need to change the name in forward propagation step and the derivative in backward propagation step. After checking I found that ReLU performs better than tanh for same alpha and same number of epochs(2000) and hidden layer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
