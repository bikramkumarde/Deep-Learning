{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression on MNIST Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run each cell in the notebook. The explanation of the cells are given on top respectively. Also, you need to have keras with tensorflow backend to load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras_team.keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloading the Mnist data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling the data so that values are between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) (60000,)\n",
      "(10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape, y_train.shape)\n",
    "print(x_test.shape, y_test.shape)\n",
    "x_train = x_train/255\n",
    "x_test = x_test/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the training and testing dataset alongwith setting the Y values to 0 or 1 depending on a condition. The condition here is last digit of VUNetId. My last digit is 1. So for the Y values of 1, new value is set to 1(true). Rest of them are set to 0(false). Also the 28x28 dataset is converted to a column vector 784x1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = x_train.reshape(60000, 784)  #Converting to column vector for each datapoint\n",
    "X1 = X\n",
    "Y = np.array(y_train == 1)\n",
    "Y = 1*Y    #Setting values to 1 or 0 for training\n",
    "\n",
    "test_X = x_test.reshape(10000,784)\n",
    "test_X = test_X.transpose()\n",
    "test_Y = np.array(y_test == 1)\n",
    "test_Y = 1*test_Y         # Setting values to 1 or 0 for test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the forward propagation step. Here W, b and the data is received as input. The operation W.X + b is performed after taking appropriate transpose. The final value is then passed through a sigmoid function to make the value between 0 - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop(W, b, data_X):\n",
    "    Wt = W.transpose()\n",
    "    intermediate = np.inner(Wt, data_X.transpose())\n",
    "    Z = intermediate + b\n",
    "    A = sigmoid(Z)\n",
    "    return A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the backward propagation step. Here the required derivatives are calculated which will be later updated in further steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_prop(A, m, data_X, data_Y):\n",
    "    dZ = A - data_Y\n",
    "    dW = np.inner(data_X, dZ)/m\n",
    "    db = sum(dZ)/m\n",
    "    return dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the sigmoid activation function used to map any real value between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computes the cost function for all the training samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function(A, m, data_Y):\n",
    "    total_cost = -(1 / m) * np.sum(data_Y * np.log(A) + (1 - data_Y) * np.log(1 - A))\n",
    "    return total_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the results are predicted. The values are predicted using test dataset and then using the actual values the accuracy is computed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "def results(W, b, data_X, data_Y):\n",
    "    Wt = W.transpose()\n",
    "    predicted = np.inner(Wt, data_X.transpose())\n",
    "    predicted = predicted + b\n",
    "    pred_Y = sigmoid(predicted)\n",
    "    pred_Y = pred_Y.transpose()\n",
    "    pred_Y = np.around(pred_Y, decimals = 0)   #Rounding off the predicted value to 0 or 1\n",
    "    accuracy = accuracy_score(data_Y, pred_Y)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the logistic regression function. First we split the training data into training and validation sets. Then we initialize W and b with random numbers. After that using W, b and the training data we implement the forward propagation. Using the results we calculate the derivatives for the backward propagation step. After that we update our b and W using the derivative values obtained. We then test this using the validation data. We store the alpha for which accuracy was the highest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Epoch: 500\n",
      "Epoch: 1000\n",
      "Alpha: 0.0001 Accuracy: 0.115\n",
      "Epoch: 0\n",
      "Epoch: 500\n",
      "Epoch: 1000\n",
      "Alpha: 0.001 Accuracy: 0.11258333333333333\n",
      "Epoch: 0\n",
      "Epoch: 500\n",
      "Epoch: 1000\n",
      "Alpha: 0.005 Accuracy: 0.84825\n",
      "Epoch: 0\n",
      "Epoch: 500\n",
      "Epoch: 1000\n",
      "Alpha: 0.01 Accuracy: 0.9405\n",
      "Epoch: 0\n",
      "Epoch: 500\n",
      "Epoch: 1000\n",
      "Alpha: 0.1 Accuracy: 0.9813333333333333\n",
      "Epoch: 0\n",
      "Epoch: 500\n",
      "Epoch: 1000\n",
      "Alpha: 0.05 Accuracy: 0.97725\n",
      "Max: 0.1\n"
     ]
    }
   ],
   "source": [
    "def logistic_reg():    \n",
    "    alpha = [0.0001, 0.001, 0.005, 0.01, 0.1, 0.05]   #set of learning rates to check from\n",
    "    epochs = 1001\n",
    "    max_cost = 0\n",
    "    max_accuracy = 0\n",
    "    alpha_max = 0\n",
    "    for i in range(len(alpha)):\n",
    "        train_X, valid_X, train_Y, valid_Y = train_test_split(X, Y, test_size=0.2) # Splitting training data using 80:20 rule\n",
    "        valid_X = valid_X.transpose()\n",
    "        train_X = train_X.transpose()\n",
    "        n, m = train_X.shape\n",
    "        W = np.random.rand(n,1)     #Initializing W and b\n",
    "        b = random.random()\n",
    "        for j in range(epochs):\n",
    "            if(j%500 == 0):\n",
    "                print('Epoch:', j)\n",
    "            A = forward_prop(W, b, train_X)\n",
    "            dW, db = backward_prop(A, m, train_X, train_Y)\n",
    "            W = W - alpha[i]*dW                    #Updating b and W\n",
    "            b = b - alpha[i]*db\n",
    "        b = sum(b)/m                  \n",
    "        accuracy = results(W, b, valid_X, valid_Y)   #testing the accuracy using validation set\n",
    "        if(accuracy >= max_accuracy):\n",
    "            alpha_max = alpha[i]                     #finding the alpha for which accuracy was the maximum\n",
    "            max_accuracy = accuracy\n",
    "        print('Alpha:',alpha[i],'Accuracy:', accuracy)\n",
    "    print( 'Max:',alpha_max)\n",
    "    return alpha_max\n",
    "alpha_max = logistic_reg()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we calculate the cost function for each iteration using the learning rate which we had obtained earlier. We run this on the entire training set. After obatining values of W and b, we calculate the training and test error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bikram Kumar De\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: RuntimeWarning: divide by zero encountered in log\n",
      "  \n",
      "C:\\Users\\Bikram Kumar De\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: RuntimeWarning: invalid value encountered in multiply\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 500\n",
      "Epoch: 1000\n"
     ]
    }
   ],
   "source": [
    "X = X.transpose()\n",
    "n, m = X.shape\n",
    "W = np.random.rand(n,1)\n",
    "b = random.random()\n",
    "epochs = 2001\n",
    "cost_array = []\n",
    "J_arr =[]\n",
    "count = 0\n",
    "for j in range(epochs):\n",
    "    if(j%500 == 0):\n",
    "        print('Epoch:', j)\n",
    "    A = forward_prop(W, b, X)\n",
    "    dW, db = backward_prop(A, m, X, Y)\n",
    "    W = W - alpha_max*dW\n",
    "    b = b - alpha_max*db\n",
    "    total_cost = cost_function(A, m, Y)\n",
    "    cost_array.append(total_cost)    # Storing the value of cost function\n",
    "    J_arr.append(count)\n",
    "    count+= 1\n",
    "b = sum(b)/m\n",
    "final_accuracy_test = results(W, b, test_X, test_Y)*100    # Calculating the training and test accuracy\n",
    "final_accuracy_train = results(W, b, X1.transpose(), Y)*100\n",
    "print('Training error:', str(100-final_accuracy_train),'%')\n",
    "print('Test error:', str(100-final_accuracy_test),'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the cost function as a function of the iterations. We can see that the cost function decreases very fast initially but after a certain number of iterations the rate of decrease is much less which indicates that our logistic regression is  converging hence the error is reducing asymptotically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Cost Function Graph')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Cost Function')\n",
    "plt.plot(J_arr, cost_array)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, you can check any individual image. You can load the particular image from the test dataset by changing the array index. The program will tell whether the number is 1 or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = x_test[2]\n",
    "plt.imshow(x1, interpolation='nearest', cmap = 'gray')\n",
    "plt.show()\n",
    "x1 = x1.reshape(784, 1)\n",
    "A = forward_prop(W, b, x1)\n",
    "if (A<0.5):\n",
    "    print('This is not 1')\n",
    "else:\n",
    "    print('This is 1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = x_test[2566]\n",
    "plt.imshow(x1, interpolation='nearest', cmap = 'gray')\n",
    "plt.show()\n",
    "x1 = x1.reshape(784, 1)\n",
    "A = forward_prop(W, b, x1)\n",
    "if (A<0.5):\n",
    "    print('This is not 1')\n",
    "else:\n",
    "    print('This is 1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Higher learning rate means the regression learns faster, i.e. it converges faster. The accuracy at end of 1000 epochs may not show significant changes on changing the learning rate because in this small dataset, having smaller alphas also makes the program converge. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
